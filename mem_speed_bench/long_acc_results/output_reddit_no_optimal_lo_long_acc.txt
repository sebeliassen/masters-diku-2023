
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 128, 'dropout': 0.5, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(602, 128)
      (1): QSAGEConv(128, 41)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.8393764495849609 sec
Run 01:
Highest Train: 98.86
Highest Valid: 96.35
  Final Train: 98.01
   Final Test: 96.34
Run 02:
Highest Train: 98.83
Highest Valid: 96.46
  Final Train: 98.79
   Final Test: 96.33
Run 03:
Highest Train: 98.89
Highest Valid: 96.43
  Final Train: 98.84
   Final Test: 96.31
Run 04:
Highest Train: 98.86
Highest Valid: 96.47
  Final Train: 98.27
   Final Test: 96.24
Run 05:
Highest Train: 98.87
Highest Valid: 96.37
  Final Train: 98.87
   Final Test: 96.34
Run 06:
Highest Train: 98.89
Highest Valid: 96.38
  Final Train: 98.65
   Final Test: 96.31
Run 07:
Highest Train: 98.86
Highest Valid: 96.47
  Final Train: 98.05
   Final Test: 96.30
Run 08:
Highest Train: 98.86
Highest Valid: 96.42
  Final Train: 98.82
   Final Test: 96.29
Run 09:
Highest Train: 98.91
Highest Valid: 96.44
  Final Train: 97.79
   Final Test: 96.32
Run 10:
Highest Train: 98.88
Highest Valid: 96.46
  Final Train: 98.03
   Final Test: 96.30
All runs:
Highest Train: 98.87 ± 0.02
Highest Valid: 96.43 ± 0.04
  Final Train: 98.41 ± 0.42
   Final Test: 96.31 ± 0.03

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 128, 'dropout': 0.5, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(602, 128)
      (1): QSAGEConv(128, 41)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 1.0837059020996094 sec
Run 01:
Highest Train: 98.84
Highest Valid: 96.49
  Final Train: 98.30
   Final Test: 96.33
Run 02:
Highest Train: 98.85
Highest Valid: 96.46
  Final Train: 98.57
   Final Test: 96.29
Run 03:
Highest Train: 98.88
Highest Valid: 96.42
  Final Train: 98.80
   Final Test: 96.30
Run 04:
Highest Train: 98.87
Highest Valid: 96.46
  Final Train: 97.47
   Final Test: 96.28
Run 05:
Highest Train: 98.86
Highest Valid: 96.46
  Final Train: 97.96
   Final Test: 96.32
Run 06:
Highest Train: 98.90
Highest Valid: 96.49
  Final Train: 98.73
   Final Test: 96.29
Run 07:
Highest Train: 98.84
Highest Valid: 96.50
  Final Train: 98.14
   Final Test: 96.30
Run 08:
Highest Train: 98.88
Highest Valid: 96.48
  Final Train: 98.51
   Final Test: 96.32
Run 09:
Highest Train: 98.85
Highest Valid: 96.43
  Final Train: 98.19
   Final Test: 96.34
Run 10:
Highest Train: 98.87
Highest Valid: 96.43
  Final Train: 98.41
   Final Test: 96.35
All runs:
Highest Train: 98.86 ± 0.02
Highest Valid: 96.46 ± 0.03
  Final Train: 98.31 ± 0.39
   Final Test: 96.31 ± 0.02

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 128, 'dropout': 0.5, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(602, 128)
      (1): QSAGEConv(128, 41)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.836174488067627 sec
Run 01:
Highest Train: 98.88
Highest Valid: 96.45
  Final Train: 98.52
   Final Test: 96.33
Run 02:
Highest Train: 98.89
Highest Valid: 96.40
  Final Train: 97.28
   Final Test: 96.30
Run 03:
Highest Train: 98.87
Highest Valid: 96.48
  Final Train: 98.33
   Final Test: 96.32
Run 04:
Highest Train: 98.89
Highest Valid: 96.44
  Final Train: 98.41
   Final Test: 96.31
Run 05:
Highest Train: 98.90
Highest Valid: 96.37
  Final Train: 98.66
   Final Test: 96.36
Run 06:
Highest Train: 98.89
Highest Valid: 96.40
  Final Train: 98.34
   Final Test: 96.39
Run 07:
Highest Train: 98.87
Highest Valid: 96.44
  Final Train: 97.83
   Final Test: 96.28
Run 08:
Highest Train: 98.88
Highest Valid: 96.41
  Final Train: 98.78
   Final Test: 96.31
Run 09:
Highest Train: 98.84
Highest Valid: 96.46
  Final Train: 98.38
   Final Test: 96.33
Run 10:
Highest Train: 98.86
Highest Valid: 96.44
  Final Train: 97.64
   Final Test: 96.30
All runs:
Highest Train: 98.88 ± 0.02
Highest Valid: 96.43 ± 0.03
  Final Train: 98.22 ± 0.48
   Final Test: 96.32 ± 0.03

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 128, 'dropout': 0.5, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(602, 128)
      (1): QSAGEConv(128, 41)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 1.135678768157959 sec
Run 01:
Highest Train: 98.85
Highest Valid: 96.38
  Final Train: 98.34
   Final Test: 96.27
Run 02:
Highest Train: 98.85
Highest Valid: 96.39
  Final Train: 98.50
   Final Test: 96.30
Run 03:
Highest Train: 98.86
Highest Valid: 96.42
  Final Train: 98.34
   Final Test: 96.24
Run 04:
Highest Train: 98.88
Highest Valid: 96.46
  Final Train: 98.22
   Final Test: 96.24
Run 05:
Highest Train: 98.86
Highest Valid: 96.48
  Final Train: 98.08
   Final Test: 96.32
Run 06:
Highest Train: 98.82
Highest Valid: 96.43
  Final Train: 98.29
   Final Test: 96.28
Run 07:
Highest Train: 98.89
Highest Valid: 96.44
  Final Train: 98.38
   Final Test: 96.32
Run 08:
Highest Train: 98.89
Highest Valid: 96.44
  Final Train: 98.81
   Final Test: 96.30
Run 09:
Highest Train: 98.89
Highest Valid: 96.41
  Final Train: 98.10
   Final Test: 96.29
Run 10:
Highest Train: 98.89
Highest Valid: 96.39
  Final Train: 98.06
   Final Test: 96.33
All runs:
Highest Train: 98.87 ± 0.03
Highest Valid: 96.42 ± 0.03
  Final Train: 98.31 ± 0.23
   Final Test: 96.29 ± 0.03

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 128, 'dropout': 0.5, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(602, 128)
      (1): QSAGEConv(128, 41)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.8438661098480225 sec
Run 01:
Highest Train: 98.89
Highest Valid: 96.42
  Final Train: 98.27
   Final Test: 96.28
Run 02:
Highest Train: 98.85
Highest Valid: 96.44
  Final Train: 97.72
   Final Test: 96.34
Run 03:
Highest Train: 98.86
Highest Valid: 96.41
  Final Train: 97.90
   Final Test: 96.29
Run 04:
Highest Train: 98.87
Highest Valid: 96.42
  Final Train: 98.02
   Final Test: 96.33
Run 05:
Highest Train: 98.85
Highest Valid: 96.44
  Final Train: 98.32
   Final Test: 96.30
Run 06:
Highest Train: 98.87
Highest Valid: 96.43
  Final Train: 98.69
   Final Test: 96.31
Run 07:
Highest Train: 98.88
Highest Valid: 96.46
  Final Train: 98.69
   Final Test: 96.37
Run 08:
Highest Train: 98.85
Highest Valid: 96.38
  Final Train: 98.18
   Final Test: 96.38
Run 09:
Highest Train: 98.86
Highest Valid: 96.48
  Final Train: 98.62
   Final Test: 96.30
Run 10:
Highest Train: 98.91
Highest Valid: 96.42
  Final Train: 98.28
   Final Test: 96.32
All runs:
Highest Train: 98.87 ± 0.02
Highest Valid: 96.43 ± 0.03
  Final Train: 98.27 ± 0.33
   Final Test: 96.32 ± 0.03
