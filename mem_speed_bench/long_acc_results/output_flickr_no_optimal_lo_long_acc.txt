
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.40241003036499023 sec
Run 01:
Highest Train: 98.37
Highest Valid: 51.87
  Final Train: 57.02
   Final Test: 51.80
Run 02:
Highest Train: 98.52
Highest Valid: 51.39
  Final Train: 61.84
   Final Test: 51.53
Run 03:
Highest Train: 98.22
Highest Valid: 51.57
  Final Train: 58.38
   Final Test: 51.72
Run 04:
Highest Train: 98.37
Highest Valid: 51.69
  Final Train: 60.85
   Final Test: 51.54
Run 05:
Highest Train: 98.64
Highest Valid: 51.82
  Final Train: 61.48
   Final Test: 51.81
Run 06:
Highest Train: 98.73
Highest Valid: 52.02
  Final Train: 57.41
   Final Test: 51.77
Run 07:
Highest Train: 98.83
Highest Valid: 52.07
  Final Train: 57.71
   Final Test: 52.01
Run 08:
Highest Train: 98.70
Highest Valid: 51.74
  Final Train: 60.29
   Final Test: 51.95
Run 09:
Highest Train: 98.91
Highest Valid: 51.46
  Final Train: 62.40
   Final Test: 51.55
Run 10:
Highest Train: 98.68
Highest Valid: 51.51
  Final Train: 57.51
   Final Test: 51.69
All runs:
Highest Train: 98.60 ± 0.22
Highest Valid: 51.71 ± 0.23
  Final Train: 59.49 ± 2.09
   Final Test: 51.74 ± 0.17

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08808159828186035 sec
Run 01:
Highest Train: 98.66
Highest Valid: 51.72
  Final Train: 57.34
   Final Test: 51.85
Run 02:
Highest Train: 98.66
Highest Valid: 51.83
  Final Train: 58.01
   Final Test: 52.02
Run 03:
Highest Train: 98.10
Highest Valid: 51.56
  Final Train: 55.80
   Final Test: 51.52
Run 04:
Highest Train: 98.74
Highest Valid: 51.28
  Final Train: 59.88
   Final Test: 51.51
Run 05:
Highest Train: 98.26
Highest Valid: 51.86
  Final Train: 58.35
   Final Test: 51.98
Run 06:
Highest Train: 98.44
Highest Valid: 51.49
  Final Train: 56.82
   Final Test: 51.94
Run 07:
Highest Train: 98.77
Highest Valid: 51.40
  Final Train: 66.48
   Final Test: 50.84
Run 08:
Highest Train: 98.17
Highest Valid: 51.52
  Final Train: 60.07
   Final Test: 52.07
Run 09:
Highest Train: 98.78
Highest Valid: 51.67
  Final Train: 57.99
   Final Test: 51.75
Run 10:
Highest Train: 97.94
Highest Valid: 51.53
  Final Train: 61.33
   Final Test: 51.45
All runs:
Highest Train: 98.45 ± 0.31
Highest Valid: 51.59 ± 0.19
  Final Train: 59.21 ± 3.04
   Final Test: 51.69 ± 0.37

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08817076683044434 sec
Run 01:
Highest Train: 97.80
Highest Valid: 51.73
  Final Train: 59.19
   Final Test: 51.92
Run 02:
Highest Train: 98.57
Highest Valid: 51.78
  Final Train: 57.53
   Final Test: 51.77
Run 03:
Highest Train: 98.88
Highest Valid: 51.55
  Final Train: 60.69
   Final Test: 51.75
Run 04:
Highest Train: 97.96
Highest Valid: 52.05
  Final Train: 59.67
   Final Test: 51.87
Run 05:
Highest Train: 98.48
Highest Valid: 51.74
  Final Train: 60.82
   Final Test: 51.83
Run 06:
Highest Train: 98.69
Highest Valid: 51.50
  Final Train: 61.52
   Final Test: 51.52
Run 07:
Highest Train: 98.30
Highest Valid: 51.52
  Final Train: 54.02
   Final Test: 51.24
Run 08:
Highest Train: 98.70
Highest Valid: 52.17
  Final Train: 60.77
   Final Test: 51.89
Run 09:
Highest Train: 98.55
Highest Valid: 51.28
  Final Train: 57.53
   Final Test: 51.61
Run 10:
Highest Train: 98.49
Highest Valid: 51.99
  Final Train: 57.44
   Final Test: 51.79
All runs:
Highest Train: 98.44 ± 0.34
Highest Valid: 51.73 ± 0.28
  Final Train: 58.92 ± 2.30
   Final Test: 51.72 ± 0.21

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08758950233459473 sec
Run 01:
Highest Train: 98.62
Highest Valid: 51.77
  Final Train: 58.46
   Final Test: 51.72
Run 02:
Highest Train: 98.29
Highest Valid: 51.71
  Final Train: 61.87
   Final Test: 51.56
Run 03:
Highest Train: 98.34
Highest Valid: 51.93
  Final Train: 59.13
   Final Test: 51.73
Run 04:
Highest Train: 98.12
Highest Valid: 51.54
  Final Train: 55.22
   Final Test: 51.52
Run 05:
Highest Train: 97.93
Highest Valid: 51.74
  Final Train: 58.96
   Final Test: 51.23
Run 06:
Highest Train: 98.24
Highest Valid: 51.76
  Final Train: 59.21
   Final Test: 51.93
Run 07:
Highest Train: 98.72
Highest Valid: 51.57
  Final Train: 59.58
   Final Test: 51.35
Run 08:
Highest Train: 98.54
Highest Valid: 51.81
  Final Train: 62.39
   Final Test: 52.10
Run 09:
Highest Train: 98.62
Highest Valid: 51.69
  Final Train: 61.22
   Final Test: 51.43
Run 10:
Highest Train: 98.60
Highest Valid: 51.75
  Final Train: 60.68
   Final Test: 51.56
All runs:
Highest Train: 98.40 ± 0.26
Highest Valid: 51.73 ± 0.11
  Final Train: 59.67 ± 2.05
   Final Test: 51.61 ± 0.26

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.0885775089263916 sec
Run 01:
Highest Train: 98.62
Highest Valid: 51.55
  Final Train: 58.61
   Final Test: 51.39
Run 02:
Highest Train: 98.56
Highest Valid: 51.51
  Final Train: 54.24
   Final Test: 51.65
Run 03:
Highest Train: 98.12
Highest Valid: 51.88
  Final Train: 60.70
   Final Test: 52.12
Run 04:
Highest Train: 98.78
Highest Valid: 51.60
  Final Train: 61.75
   Final Test: 51.63
Run 05:
Highest Train: 98.73
Highest Valid: 51.47
  Final Train: 56.11
   Final Test: 51.73
Run 06:
Highest Train: 98.62
Highest Valid: 51.26
  Final Train: 63.23
   Final Test: 51.19
Run 07:
Highest Train: 98.71
Highest Valid: 51.47
  Final Train: 60.57
   Final Test: 51.68
Run 08:
Highest Train: 98.35
Highest Valid: 51.64
  Final Train: 60.66
   Final Test: 51.45
Run 09:
Highest Train: 98.65
Highest Valid: 51.56
  Final Train: 60.55
   Final Test: 51.77
Run 10:
Highest Train: 98.32
Highest Valid: 51.86
  Final Train: 57.15
   Final Test: 51.95
All runs:
Highest Train: 98.55 ± 0.21
Highest Valid: 51.58 ± 0.18
  Final Train: 59.36 ± 2.78
   Final Test: 51.66 ± 0.27
