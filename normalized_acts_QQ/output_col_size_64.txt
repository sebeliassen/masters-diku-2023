==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.37
Highest Valid: 72.36
  Final Train: 75.50
   Final Test: 71.39
Run 02:
Highest Train: 76.49
Highest Valid: 72.35
  Final Train: 76.31
   Final Test: 71.11
Run 03:
Highest Train: 76.30
Highest Valid: 72.47
  Final Train: 75.38
   Final Test: 71.28
Run 04:
Highest Train: 76.14
Highest Valid: 72.25
  Final Train: 75.65
   Final Test: 71.61
Run 05:
Highest Train: 75.94
Highest Valid: 72.43
  Final Train: 74.84
   Final Test: 71.23
Run 06:
Highest Train: 76.33
Highest Valid: 72.30
  Final Train: 75.71
   Final Test: 71.18
Run 07:
Highest Train: 76.21
Highest Valid: 72.65
  Final Train: 76.19
   Final Test: 71.78
Run 08:
Highest Train: 76.35
Highest Valid: 72.40
  Final Train: 76.35
   Final Test: 70.60
Run 09:
Highest Train: 76.17
Highest Valid: 72.23
  Final Train: 75.87
   Final Test: 70.34
Run 10:
Highest Train: 76.39
Highest Valid: 72.60
  Final Train: 75.98
   Final Test: 71.24
All runs:
Highest Train: 76.27 ± 0.16
Highest Valid: 72.40 ± 0.14
  Final Train: 75.78 ± 0.47
   Final Test: 71.17 ± 0.43
