==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.62
Highest Valid: 72.37
  Final Train: 76.62
   Final Test: 70.96
Run 02:
Highest Train: 76.68
Highest Valid: 72.58
  Final Train: 76.10
   Final Test: 71.05
Run 03:
Highest Train: 76.65
Highest Valid: 72.59
  Final Train: 76.26
   Final Test: 71.48
Run 04:
Highest Train: 76.47
Highest Valid: 72.48
  Final Train: 76.38
   Final Test: 71.30
Run 05:
Highest Train: 76.65
Highest Valid: 72.42
  Final Train: 76.29
   Final Test: 71.28
Run 06:
Highest Train: 76.76
Highest Valid: 72.39
  Final Train: 76.35
   Final Test: 71.20
Run 07:
Highest Train: 76.67
Highest Valid: 72.51
  Final Train: 75.53
   Final Test: 70.82
Run 08:
Highest Train: 76.66
Highest Valid: 72.61
  Final Train: 76.60
   Final Test: 71.25
Run 09:
Highest Train: 76.64
Highest Valid: 72.43
  Final Train: 75.84
   Final Test: 71.49
Run 10:
Highest Train: 76.55
Highest Valid: 72.74
  Final Train: 76.25
   Final Test: 70.99
All runs:
Highest Train: 76.64 ± 0.08
Highest Valid: 72.51 ± 0.12
  Final Train: 76.22 ± 0.33
   Final Test: 71.18 ± 0.22
