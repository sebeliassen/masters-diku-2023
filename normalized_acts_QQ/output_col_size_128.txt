==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.15
Highest Valid: 72.41
  Final Train: 75.72
   Final Test: 71.30
Run 02:
Highest Train: 76.04
Highest Valid: 72.45
  Final Train: 75.65
   Final Test: 71.46
Run 03:
Highest Train: 76.19
Highest Valid: 72.64
  Final Train: 76.17
   Final Test: 71.20
Run 04:
Highest Train: 76.26
Highest Valid: 72.30
  Final Train: 76.23
   Final Test: 70.85
Run 05:
Highest Train: 76.19
Highest Valid: 72.41
  Final Train: 76.00
   Final Test: 70.95
Run 06:
Highest Train: 76.07
Highest Valid: 72.21
  Final Train: 75.44
   Final Test: 70.96
Run 07:
Highest Train: 76.07
Highest Valid: 72.48
  Final Train: 76.00
   Final Test: 71.27
Run 08:
Highest Train: 76.13
Highest Valid: 72.39
  Final Train: 75.32
   Final Test: 70.99
Run 09:
Highest Train: 76.02
Highest Valid: 72.42
  Final Train: 75.64
   Final Test: 71.20
Run 10:
Highest Train: 76.07
Highest Valid: 72.16
  Final Train: 75.87
   Final Test: 70.75
All runs:
Highest Train: 76.12 ± 0.08
Highest Valid: 72.39 ± 0.14
  Final Train: 75.80 ± 0.30
   Final Test: 71.09 ± 0.23
