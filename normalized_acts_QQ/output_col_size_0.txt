==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.70
Highest Valid: 72.49
  Final Train: 75.52
   Final Test: 71.44
Run 02:
Highest Train: 76.42
Highest Valid: 72.51
  Final Train: 76.36
   Final Test: 71.45
Run 03:
Highest Train: 76.63
Highest Valid: 72.55
  Final Train: 76.62
   Final Test: 71.05
Run 04:
Highest Train: 76.67
Highest Valid: 72.80
  Final Train: 76.21
   Final Test: 71.14
Run 05:
Highest Train: 76.68
Highest Valid: 72.57
  Final Train: 75.99
   Final Test: 70.86
Run 06:
Highest Train: 76.48
Highest Valid: 72.42
  Final Train: 76.26
   Final Test: 71.09
Run 07:
Highest Train: 76.62
Highest Valid: 72.82
  Final Train: 76.57
   Final Test: 71.43
Run 08:
Highest Train: 76.73
Highest Valid: 72.62
  Final Train: 76.20
   Final Test: 71.74
Run 09:
Highest Train: 76.79
Highest Valid: 72.52
  Final Train: 76.76
   Final Test: 70.84
Run 10:
Highest Train: 76.67
Highest Valid: 72.38
  Final Train: 75.90
   Final Test: 71.38
All runs:
Highest Train: 76.64 ± 0.11
Highest Valid: 72.57 ± 0.15
  Final Train: 76.24 ± 0.37
   Final Test: 71.24 ± 0.29
