
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.34926843643188477 sec

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.0877385139465332 sec

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.0872032642364502 sec

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08751487731933594 sec

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08848047256469727 sec

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.328371524810791 sec
Run 01:
Highest Train: 98.54
Highest Valid: 51.68
  Final Train: 57.48
   Final Test: 51.80
Run 02:
Highest Train: 98.73
Highest Valid: 52.10
  Final Train: 61.15
   Final Test: 51.70
Run 03:
Highest Train: 98.58
Highest Valid: 51.81
  Final Train: 61.38
   Final Test: 51.72
Run 04:
Highest Train: 98.62
Highest Valid: 51.85
  Final Train: 62.11
   Final Test: 51.68
Run 05:
Highest Train: 98.84
Highest Valid: 51.86
  Final Train: 57.62
   Final Test: 51.59
All runs:
Highest Train: 98.66 ± 0.12
Highest Valid: 51.86 ± 0.15
  Final Train: 59.95 ± 2.22
   Final Test: 51.70 ± 0.07

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08710289001464844 sec
Run 01:
Highest Train: 98.75
Highest Valid: 51.51
  Final Train: 59.74
   Final Test: 51.64
Run 02:
Highest Train: 98.46
Highest Valid: 51.79
  Final Train: 60.43
   Final Test: 51.85
Run 03:
Highest Train: 98.80
Highest Valid: 52.04
  Final Train: 61.22
   Final Test: 51.66
Run 04:
Highest Train: 98.52
Highest Valid: 51.74
  Final Train: 61.69
   Final Test: 51.58
Run 05:
Highest Train: 98.41
Highest Valid: 51.84
  Final Train: 60.04
   Final Test: 51.85
All runs:
Highest Train: 98.59 ± 0.17
Highest Valid: 51.78 ± 0.19
  Final Train: 60.63 ± 0.81
   Final Test: 51.72 ± 0.12

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08704471588134766 sec
Run 01:
Highest Train: 98.58
Highest Valid: 51.76
  Final Train: 57.83
   Final Test: 51.80
Run 02:
Highest Train: 98.47
Highest Valid: 52.01
  Final Train: 60.53
   Final Test: 51.72
Run 03:
Highest Train: 98.67
Highest Valid: 51.45
  Final Train: 54.54
   Final Test: 51.43
Run 04:
Highest Train: 98.55
Highest Valid: 51.90
  Final Train: 59.48
   Final Test: 51.91
Run 05:
Highest Train: 98.21
Highest Valid: 51.61
  Final Train: 61.56
   Final Test: 51.78
All runs:
Highest Train: 98.50 ± 0.17
Highest Valid: 51.75 ± 0.22
  Final Train: 58.79 ± 2.75
   Final Test: 51.73 ± 0.18

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08773922920227051 sec
Run 01:
Highest Train: 98.34
Highest Valid: 51.73
  Final Train: 59.72
   Final Test: 51.82
Run 02:
Highest Train: 98.79
Highest Valid: 51.31
  Final Train: 54.24
   Final Test: 51.38
Run 03:
Highest Train: 98.63
Highest Valid: 51.50
  Final Train: 61.06
   Final Test: 51.43
Run 04:
Highest Train: 98.31
Highest Valid: 51.81
  Final Train: 56.47
   Final Test: 51.86
Run 05:
Highest Train: 98.34
Highest Valid: 51.70
  Final Train: 58.15
   Final Test: 51.99
All runs:
Highest Train: 98.48 ± 0.22
Highest Valid: 51.61 ± 0.20
  Final Train: 57.93 ± 2.68
   Final Test: 51.70 ± 0.27

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
model config: {'arch_name': 'SAGE', 'architecture': {'num_layers': 2, 'hidden_channels': 256, 'dropout': 0.3, 'batch_norm': True, 'residual': False}, 'optim': 'adam', 'lr': 0.01, 'epochs': 400, 'name': 'SAGE', 'loop': False, 'normalize': False}
clipping grad norm: 0.5
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
Use GPU 0 for training
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(500, 256)
      (1): QSAGEConv(256, 7)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
converting data form...
done. used 0.08742594718933105 sec
Run 01:
Highest Train: 98.58
Highest Valid: 51.71
  Final Train: 60.30
   Final Test: 51.93
Run 02:
Highest Train: 98.67
Highest Valid: 51.75
  Final Train: 61.69
   Final Test: 51.82
Run 03:
Highest Train: 98.77
Highest Valid: 51.93
  Final Train: 58.20
   Final Test: 51.91
Run 04:
Highest Train: 98.28
Highest Valid: 51.68
  Final Train: 59.23
   Final Test: 51.70
Run 05:
Highest Train: 98.55
Highest Valid: 51.34
  Final Train: 62.70
   Final Test: 51.60
All runs:
Highest Train: 98.57 ± 0.19
Highest Valid: 51.68 ± 0.21
  Final Train: 60.42 ± 1.82
   Final Test: 51.79 ± 0.14
