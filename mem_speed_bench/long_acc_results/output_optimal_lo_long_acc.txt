
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.59
Highest Valid: 72.75
  Final Train: 76.40
   Final Test: 71.26
Run 02:
Highest Train: 76.61
Highest Valid: 72.47
  Final Train: 76.55
   Final Test: 71.33
Run 03:
Highest Train: 76.60
Highest Valid: 72.42
  Final Train: 75.75
   Final Test: 71.13
Run 04:
Highest Train: 76.76
Highest Valid: 72.55
  Final Train: 76.49
   Final Test: 71.85
Run 05:
Highest Train: 76.46
Highest Valid: 72.64
  Final Train: 75.82
   Final Test: 71.78
All runs:
Highest Train: 76.60 ± 0.11
Highest Valid: 72.57 ± 0.13
  Final Train: 76.20 ± 0.39
   Final Test: 71.47 ± 0.32

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.58
Highest Valid: 72.66
  Final Train: 76.28
   Final Test: 71.14
Run 02:
Highest Train: 76.48
Highest Valid: 72.67
  Final Train: 76.48
   Final Test: 71.36
Run 03:
Highest Train: 76.65
Highest Valid: 72.54
  Final Train: 76.53
   Final Test: 71.72
Run 04:
Highest Train: 76.67
Highest Valid: 72.55
  Final Train: 76.47
   Final Test: 71.01
Run 05:
Highest Train: 76.65
Highest Valid: 72.41
  Final Train: 76.57
   Final Test: 71.00
All runs:
Highest Train: 76.61 ± 0.08
Highest Valid: 72.56 ± 0.11
  Final Train: 76.47 ± 0.11
   Final Test: 71.25 ± 0.30

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.56
Highest Valid: 72.50
  Final Train: 76.43
   Final Test: 71.24
Run 02:
Highest Train: 76.51
Highest Valid: 72.38
  Final Train: 76.34
   Final Test: 71.29
Run 03:
Highest Train: 76.61
Highest Valid: 72.58
  Final Train: 76.18
   Final Test: 71.41
Run 04:
Highest Train: 76.72
Highest Valid: 72.61
  Final Train: 76.54
   Final Test: 71.47
Run 05:
Highest Train: 76.57
Highest Valid: 72.38
  Final Train: 75.87
   Final Test: 71.41
All runs:
Highest Train: 76.59 ± 0.08
Highest Valid: 72.49 ± 0.11
  Final Train: 76.27 ± 0.26
   Final Test: 71.36 ± 0.10

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.78
Highest Valid: 72.35
  Final Train: 76.28
   Final Test: 70.58
Run 02:
Highest Train: 76.44
Highest Valid: 72.43
  Final Train: 76.34
   Final Test: 71.24
Run 03:
Highest Train: 76.70
Highest Valid: 72.33
  Final Train: 76.30
   Final Test: 70.90
Run 04:
Highest Train: 76.47
Highest Valid: 72.25
  Final Train: 76.17
   Final Test: 71.12
Run 05:
Highest Train: 76.47
Highest Valid: 72.43
  Final Train: 75.91
   Final Test: 71.38
All runs:
Highest Train: 76.57 ± 0.16
Highest Valid: 72.36 ± 0.08
  Final Train: 76.20 ± 0.18
   Final Test: 71.05 ± 0.31

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.82
Highest Valid: 72.52
  Final Train: 76.77
   Final Test: 71.59
Run 02:
Highest Train: 76.79
Highest Valid: 72.59
  Final Train: 76.72
   Final Test: 70.71
Run 03:
Highest Train: 76.49
Highest Valid: 72.57
  Final Train: 76.29
   Final Test: 71.44
Run 04:
Highest Train: 76.60
Highest Valid: 72.58
  Final Train: 76.46
   Final Test: 70.82
Run 05:
Highest Train: 76.42
Highest Valid: 72.11
  Final Train: 74.89
   Final Test: 71.51
All runs:
Highest Train: 76.62 ± 0.18
Highest Valid: 72.47 ± 0.20
  Final Train: 76.23 ± 0.77
   Final Test: 71.21 ± 0.41

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.66
Highest Valid: 72.47
  Final Train: 76.07
   Final Test: 71.59
Run 02:
Highest Train: 76.41
Highest Valid: 72.56
  Final Train: 75.70
   Final Test: 71.48
Run 03:
Highest Train: 76.19
Highest Valid: 72.58
  Final Train: 75.93
   Final Test: 71.34
Run 04:
Highest Train: 76.83
Highest Valid: 72.57
  Final Train: 76.56
   Final Test: 71.45
Run 05:
Highest Train: 76.81
Highest Valid: 72.49
  Final Train: 76.81
   Final Test: 70.73
All runs:
Highest Train: 76.58 ± 0.28
Highest Valid: 72.53 ± 0.05
  Final Train: 76.21 ± 0.46
   Final Test: 71.32 ± 0.34

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.51
Highest Valid: 72.50
  Final Train: 76.34
   Final Test: 71.30
Run 02:
Highest Train: 76.75
Highest Valid: 72.53
  Final Train: 76.48
   Final Test: 71.65
Run 03:
Highest Train: 76.40
Highest Valid: 72.49
  Final Train: 76.29
   Final Test: 71.18
Run 04:
Highest Train: 76.64
Highest Valid: 72.43
  Final Train: 76.63
   Final Test: 70.99
Run 05:
Highest Train: 76.55
Highest Valid: 72.44
  Final Train: 76.17
   Final Test: 71.17
All runs:
Highest Train: 76.57 ± 0.13
Highest Valid: 72.48 ± 0.04
  Final Train: 76.38 ± 0.18
   Final Test: 71.26 ± 0.24

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.29
Highest Valid: 72.35
  Final Train: 75.53
   Final Test: 71.34
Run 02:
Highest Train: 76.98
Highest Valid: 72.61
  Final Train: 76.40
   Final Test: 71.15
Run 03:
Highest Train: 76.66
Highest Valid: 72.51
  Final Train: 76.50
   Final Test: 71.51
Run 04:
Highest Train: 76.61
Highest Valid: 72.54
  Final Train: 76.56
   Final Test: 71.07
Run 05:
Highest Train: 76.75
Highest Valid: 72.61
  Final Train: 76.16
   Final Test: 71.13
All runs:
Highest Train: 76.66 ± 0.25
Highest Valid: 72.52 ± 0.11
  Final Train: 76.23 ± 0.42
   Final Test: 71.24 ± 0.18

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.72
Highest Valid: 72.37
  Final Train: 76.62
   Final Test: 71.06
Run 02:
Highest Train: 76.80
Highest Valid: 72.38
  Final Train: 76.28
   Final Test: 71.60
Run 03:
Highest Train: 76.50
Highest Valid: 72.35
  Final Train: 76.29
   Final Test: 71.00
Run 04:
Highest Train: 76.49
Highest Valid: 72.40
  Final Train: 75.86
   Final Test: 71.83
Run 05:
Highest Train: 76.75
Highest Valid: 72.66
  Final Train: 76.62
   Final Test: 71.31
All runs:
Highest Train: 76.65 ± 0.14
Highest Valid: 72.43 ± 0.13
  Final Train: 76.33 ± 0.31
   Final Test: 71.36 ± 0.36

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.65
Highest Valid: 72.45
  Final Train: 76.32
   Final Test: 70.92
Run 02:
Highest Train: 76.56
Highest Valid: 72.63
  Final Train: 76.36
   Final Test: 71.58
Run 03:
Highest Train: 76.67
Highest Valid: 72.40
  Final Train: 76.51
   Final Test: 70.92
Run 04:
Highest Train: 76.41
Highest Valid: 72.28
  Final Train: 76.32
   Final Test: 71.24
Run 05:
Highest Train: 76.54
Highest Valid: 72.32
  Final Train: 75.87
   Final Test: 71.29
All runs:
Highest Train: 76.57 ± 0.10
Highest Valid: 72.41 ± 0.14
  Final Train: 76.28 ± 0.24
   Final Test: 71.19 ± 0.28

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.73
Highest Valid: 72.52
  Final Train: 76.73
   Final Test: 71.17
Run 02:
Highest Train: 76.55
Highest Valid: 72.48
  Final Train: 76.07
   Final Test: 71.17
Run 03:
Highest Train: 76.57
Highest Valid: 72.27
  Final Train: 76.31
   Final Test: 70.99
Run 04:
Highest Train: 76.74
Highest Valid: 72.32
  Final Train: 76.74
   Final Test: 70.94
Run 05:
Highest Train: 76.48
Highest Valid: 72.46
  Final Train: 76.34
   Final Test: 70.96
All runs:
Highest Train: 76.62 ± 0.12
Highest Valid: 72.41 ± 0.11
  Final Train: 76.44 ± 0.29
   Final Test: 71.04 ± 0.12

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.72
Highest Valid: 72.62
  Final Train: 76.25
   Final Test: 71.40
Run 02:
Highest Train: 76.62
Highest Valid: 72.56
  Final Train: 75.64
   Final Test: 71.21
Run 03:
Highest Train: 76.61
Highest Valid: 72.58
  Final Train: 76.58
   Final Test: 71.00
Run 04:
Highest Train: 76.62
Highest Valid: 72.32
  Final Train: 76.34
   Final Test: 70.36
Run 05:
Highest Train: 76.44
Highest Valid: 72.47
  Final Train: 76.31
   Final Test: 71.12
All runs:
Highest Train: 76.60 ± 0.10
Highest Valid: 72.51 ± 0.12
  Final Train: 76.23 ± 0.35
   Final Test: 71.02 ± 0.40

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.69
Highest Valid: 72.45
  Final Train: 76.09
   Final Test: 71.07
Run 02:
Highest Train: 76.68
Highest Valid: 72.50
  Final Train: 76.27
   Final Test: 70.98
Run 03:
Highest Train: 76.43
Highest Valid: 72.37
  Final Train: 75.83
   Final Test: 71.08
Run 04:
Highest Train: 76.68
Highest Valid: 72.60
  Final Train: 75.94
   Final Test: 71.09
Run 05:
Highest Train: 76.52
Highest Valid: 72.55
  Final Train: 76.20
   Final Test: 71.54
All runs:
Highest Train: 76.60 ± 0.12
Highest Valid: 72.50 ± 0.09
  Final Train: 76.07 ± 0.18
   Final Test: 71.15 ± 0.22

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.47
Highest Valid: 72.06
  Final Train: 76.22
   Final Test: 70.30
Run 02:
Highest Train: 76.73
Highest Valid: 72.46
  Final Train: 76.49
   Final Test: 70.77
Run 03:
Highest Train: 76.32
Highest Valid: 72.59
  Final Train: 76.30
   Final Test: 71.03
Run 04:
Highest Train: 76.80
Highest Valid: 72.61
  Final Train: 76.62
   Final Test: 71.57
Run 05:
Highest Train: 76.75
Highest Valid: 72.62
  Final Train: 76.17
   Final Test: 71.60
All runs:
Highest Train: 76.61 ± 0.21
Highest Valid: 72.47 ± 0.24
  Final Train: 76.36 ± 0.19
   Final Test: 71.05 ± 0.55

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.54
Highest Valid: 72.30
  Final Train: 76.31
   Final Test: 71.23
Run 02:
Highest Train: 76.53
Highest Valid: 72.29
  Final Train: 76.51
   Final Test: 70.61
Run 03:
Highest Train: 76.43
Highest Valid: 72.41
  Final Train: 76.02
   Final Test: 71.32
Run 04:
Highest Train: 76.62
Highest Valid: 72.45
  Final Train: 76.52
   Final Test: 70.88
Run 05:
Highest Train: 76.89
Highest Valid: 72.71
  Final Train: 76.89
   Final Test: 71.23
All runs:
Highest Train: 76.60 ± 0.18
Highest Valid: 72.43 ± 0.17
  Final Train: 76.45 ± 0.32
   Final Test: 71.05 ± 0.30

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.71
Highest Valid: 72.48
  Final Train: 76.65
   Final Test: 70.90
Run 02:
Highest Train: 76.67
Highest Valid: 72.52
  Final Train: 76.27
   Final Test: 71.50
Run 03:
Highest Train: 76.55
Highest Valid: 72.29
  Final Train: 76.39
   Final Test: 70.95
Run 04:
Highest Train: 76.84
Highest Valid: 72.71
  Final Train: 76.59
   Final Test: 71.27
Run 05:
Highest Train: 76.54
Highest Valid: 72.29
  Final Train: 76.24
   Final Test: 71.34
All runs:
Highest Train: 76.66 ± 0.12
Highest Valid: 72.46 ± 0.18
  Final Train: 76.43 ± 0.19
   Final Test: 71.19 ± 0.26

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.59
Highest Valid: 72.42
  Final Train: 76.45
   Final Test: 70.84
Run 02:
Highest Train: 76.86
Highest Valid: 72.56
  Final Train: 76.36
   Final Test: 71.75
Run 03:
Highest Train: 76.82
Highest Valid: 72.70
  Final Train: 76.53
   Final Test: 71.36
Run 04:
Highest Train: 76.67
Highest Valid: 72.37
  Final Train: 75.50
   Final Test: 71.59
Run 05:
Highest Train: 76.62
Highest Valid: 72.54
  Final Train: 76.54
   Final Test: 70.63
All runs:
Highest Train: 76.71 ± 0.12
Highest Valid: 72.52 ± 0.13
  Final Train: 76.28 ± 0.44
   Final Test: 71.23 ± 0.48

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.58
Highest Valid: 72.63
  Final Train: 76.43
   Final Test: 71.45
Run 02:
Highest Train: 76.83
Highest Valid: 72.54
  Final Train: 76.46
   Final Test: 71.61
Run 03:
Highest Train: 76.72
Highest Valid: 72.59
  Final Train: 76.72
   Final Test: 71.68
Run 04:
Highest Train: 76.69
Highest Valid: 72.55
  Final Train: 76.17
   Final Test: 70.91
Run 05:
Highest Train: 76.56
Highest Valid: 72.54
  Final Train: 76.36
   Final Test: 71.68
All runs:
Highest Train: 76.68 ± 0.11
Highest Valid: 72.57 ± 0.04
  Final Train: 76.43 ± 0.20
   Final Test: 71.47 ± 0.32

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.45
Highest Valid: 72.38
  Final Train: 75.80
   Final Test: 70.83
Run 02:
Highest Train: 76.56
Highest Valid: 72.57
  Final Train: 76.26
   Final Test: 71.59
Run 03:
Highest Train: 76.51
Highest Valid: 72.51
  Final Train: 75.99
   Final Test: 71.28
Run 04:
Highest Train: 76.54
Highest Valid: 72.36
  Final Train: 75.95
   Final Test: 71.02
Run 05:
Highest Train: 76.56
Highest Valid: 72.57
  Final Train: 76.48
   Final Test: 71.43
All runs:
Highest Train: 76.53 ± 0.04
Highest Valid: 72.48 ± 0.10
  Final Train: 76.10 ± 0.27
   Final Test: 71.23 ± 0.31

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/exact/lib/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 111
CUDA SETUP: Loading binary /opt/conda/envs/exact/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...
==============================Store activation in INT2==============================
use qmodule: True
kept frac: 0.125
amp mode: False
use the custom loss function
Use GPU 0 for training
clipping grad norm: 0.5
convert the model
QModule(
  (model): SAGE(
    (dropout): QDropout()
    (activation): QReLU()
    (convs): ModuleList(
      (0): QSAGEConv(128, 128)
      (1): QSAGEConv(128, 128)
      (2): QSAGEConv(128, 40)
    )
    (bns): ModuleList(
      (0): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): QBatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
Run 01:
Highest Train: 76.59
Highest Valid: 72.33
  Final Train: 75.81
   Final Test: 70.68
Run 02:
Highest Train: 76.48
Highest Valid: 72.41
  Final Train: 76.28
   Final Test: 71.33
Run 03:
Highest Train: 76.26
Highest Valid: 72.31
  Final Train: 75.72
   Final Test: 71.01
Run 04:
Highest Train: 76.63
Highest Valid: 72.46
  Final Train: 76.63
   Final Test: 70.63
Run 05:
Highest Train: 76.62
Highest Valid: 72.49
  Final Train: 76.55
   Final Test: 71.21
All runs:
Highest Train: 76.52 ± 0.15
Highest Valid: 72.40 ± 0.07
  Final Train: 76.20 ± 0.42
   Final Test: 70.97 ± 0.31
